<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CS180</title>
    <style>
        :root {
            --gap: 10px;
        }
        .gallery {
            display: grid;
            grid-template-columns: repeat(6, 1fr);
            gap: var(--gap);
            margin-bottom: 10px;
            margin-inline: 25vw;
        }
        .gallery img {
            width: 100%;
        }
        .gallery figure {
            margin: 0;
        }
        .gallery figcaption {
            text-align: center;
        }
        .gallery_1 {
            display: grid;
            grid-template-columns: repeat(1, 1fr);
            gap: var(--gap);
            margin-bottom: 10px;
            margin-inline: 25vw;
        }
        .gallery_1 img {
            width: 100%;
        }
        .gallery_1 figure {
            margin: 0;
        }
        .gallery_1 figcaption {
            text-align: center;
        }
        .gallery_2 {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: var(--gap);
            margin-bottom: 10px;
            margin-inline: 25vw;
        }
        .gallery_2 img {
            width: 100%;
        }
        .gallery_2 figure {
            margin: 0;
        }
        .gallery_2 figcaption {
            text-align: center;
        }
    </style>
</head>
<body>
    <h1>Project 6 - Cui, Jiangren</h1>
    <h2>Neural Radiance Fields</h2>
    <h3>Part A</h3>
    <h4>Network</h4>
    <p>In order to fit a two-dimensional image through a neural field, the coordinates input to the neural network need to be expanded. Specifically, the coordinates themselves only contain information about the position. The neural network can fit large color blocks well through this information, but it is difficult to fit details such as textures directly through this information. The sine and cosine functions are related to the periodicity in space. The normalized coordinates are processed by the sine and cosine functions, which directly reflect the periodic characteristics of the coordinates, making it easier for the neural network to express the details of the image.</p>
    <p>The following network is used to process the image, the input is the expanded coordinates, and the output is the RGB value.</p>
    <div class="gallery_1">
        <img src="media/mlp_img.jpg">
    </div>
    <h4>Dataloader</h4>
    <p>Because the neural network has a one-to-one correspondence with the image, all the data is obtained from a single image. In order to process large-resolution images, a data loader is implemented that extracts a portion of pixels from the image as a batch for training.</p>
    <p>My data loader returns data corresponding to 10k pixels each time. For the selected images, I perform about 70 gradient descents per epoch, for a total of 25 epochs, or about 1,750 cycles.</p>
    <h4>Loss Function, Optimizer, and Metric</h4>
    <p>Below are the loss and psnr curves for lr=1e-2, L=10, and a visualization of the training process.</p>
    <div class="gallery_2">
        <img src="media/fox_l.png">
        <img src="media/fox_p.png">
        <img src="media/image.jpg">
        <img src="media/fox.gif">
    </div>
    <div class="gallery">
        <figure>
            <img src="media/image.jpg">
            <figcaption>original</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_0.png">
            <figcaption>epoch 1</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_2.png">
            <figcaption>epoch 3</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_4.png">
            <figcaption>epoch 5</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_14.png">
            <figcaption>epoch 15</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_24.png">
            <figcaption>epoch 25</figcaption>
        </figure>
    </div>
    <h4>Hyperparameter Tuning</h4>
    <p>Below are the loss curve and psnr curve when lr=1e-2, L=5, as well as the visualization of the training process. It is easy to find that the fitting effect for high-frequency components is not good, which is very obvious near the eyes.</p>
    <div class="gallery_2">
        <img src="media/fox_1_l.png">
        <img src="media/fox_1_p.png">
        <img src="media/image.jpg">
        <img src="media/fox_1.gif">
    </div>
    <div class="gallery">
        <figure>
            <img src="media/image.jpg">
            <figcaption>original</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_1_0.png">
            <figcaption>epoch 1</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_1_2.png">
            <figcaption>epoch 3</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_1_4.png">
            <figcaption>epoch 5</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_1_14.png">
            <figcaption>epoch 15</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_1_24.png">
            <figcaption>epoch 25</figcaption>
        </figure>
    </div>
    <p>Below are the loss and psnr curves for lr=1e-3, L=10, and a visualization of the training process. Lowering the learning rate makes the fit slower but the final loss is smaller. The change in loss here is not very large.</p>
    <div class="gallery_2">
        <img src="media/fox_2_l.png">
        <img src="media/fox_2_p.png">
        <img src="media/image.jpg">
        <img src="media/fox_2.gif">
    </div>
    <div class="gallery">
        <figure>
            <img src="media/image.jpg">
            <figcaption>original</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_2_0.png">
            <figcaption>epoch 1</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_2_2.png">
            <figcaption>epoch 3</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_2_4.png">
            <figcaption>epoch 5</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_2_14.png">
            <figcaption>epoch 15</figcaption>
        </figure>
        <figure>
            <img src="media/more/f_2_24.png">
            <figcaption>epoch 25</figcaption>
        </figure>
    </div>
    <p>We use lr=1e-2, L=10 as parameters and fit another image. Below are the loss curve and psnr curve, as well as a visualization of the training process.</p>
    <div class="gallery_2">
        <img src="media/school_l.png">
        <img src="media/school_p.png">
        <img src="media/school.jpg">
        <img src="media/school.gif">
    </div>
    <div class="gallery">
        <figure>
            <img src="media/school.jpg">
            <figcaption>original</figcaption>
        </figure>
        <figure>
            <img src="media/more/s_0.png">
            <figcaption>epoch 1</figcaption>
        </figure>
        <figure>
            <img src="media/more/s_2.png">
            <figcaption>epoch 3</figcaption>
        </figure>
        <figure>
            <img src="media/more/s_4.png">
            <figcaption>epoch 5</figcaption>
        </figure>
        <figure>
            <img src="media/more/s_14.png">
            <figcaption>epoch 15</figcaption>
        </figure>
        <figure>
            <img src="media/more/s_24.png">
            <figcaption>epoch 25</figcaption>
        </figure>
    </div>
    <h3>Part B</h3>
    <h4>Create Rays from Cameras</h4>
    <p>Neural radiance field fits objects from multi-view images. For this experiment, the parameters corresponding to each image are given by the dataset. The neural network accepts the 3D coordinates and direction of the camera as input, and outputs density and RGB values. The RGB value of each point in the final image is obtained by accumulating the RGB values of all points through which the ray passes according to the density.</p>
    <p>First, create the ray. There are three steps to create the ray. The first step is to convert the coordinates of the point in the camera into the world coordinates. The second step is to convert the pixel coordinates of the point into the coordinates in the camera. The third step is to get the starting point and direction of the ray corresponding to the pixel. These can be directly implemented according to the formulas given on the homework website.</p>
    <h4>Sampling</h4>
    <p>Then we implement sampling. Sampling is divided into sampling rays from images and sampling points from rays. For the former, global sampling is always performed in this experiment, that is, sampling from all images in the training set. For the latter, 64 points are sampled from each ray in this experiment.</p>
    <h4>Putting the Dataloading All Together</h4>
    <p>Here we test the data loader as per the job website. Below is the visualization.</p>
    <div class="gallery_1">
        <img src="media/render.png">
    </div>
    <h4>Neural Radiance Field</h4>
    <p>In general, the network implementation of the neural radiation field is similar to that in the first part, but a more complex structure is adopted to handle more complex data. The specific structure is as follows.</p>
    <div class="gallery_1">
        <img src="media/mlp_nerf.png">
    </div>
    <h4>Volume Rendering</h4>
    <p>The last step is volume rendering. Volume rendering is to get the final color from the rays by superimposing RGB according to the density. The homework website gives an approximate formula, which can be directly accumulated.</p>
    <p>There is one additional summary for the whole process. The coordinates input to the neural network are obtained by sampling the rays corresponding to the pixel points. The density and RGB output of the neural network still need to be generated through volume rendering to predict the pixel RGB value and finally get the loss. In other words, although it is intuitively easy to think that the pixel coordinates are input to get the pixel value, in fact, for the neural network, the input is the ray sampling point, and the output is the density and RGB value of each sampling point. The back propagation of gradients is not only related to the internal operations of the neural network.</p>
    <p>In this experiment, we selected lr=5e-4, L(x)=10 , L(d)=4, batch=10k as parameters. After testing, we found that at 1k epochs given in the homework website, it is only possible to achieve a PSNR of 23 on the training set. This may be because global sampling cannot guarantee that each image is uniformly weighted in the early stage. However, at 2k epochs, the PSNR on both the training set and the validation set is higher than the required value.</p>
    <p>Below are the loss curve and PSNR curve, as well as the visualization of the training process. The orange line is the data obtained on a picture in the validation set. Since a picture contains 40k pixels , it takes too much time to test the loss on the entire validation set during training, so only one picture is selected as a reference. There is one more detail to note about training, that is, the background color must be black during training, but other colors can be used to generate images and videos during testing.</p>
    <div class="gallery_2">
        <img src="media/lego_l.png">
        <img src="media/lego_p.png">
        <img src="media/lego_v_s.gif">
        <img src="media/lego_dp_v_s.gif">
    </div>
    <div class="gallery">
        <figure>
            <img src="media/more/val.png">
            <figcaption>original</figcaption>
        </figure>
        <figure>
            <img src="media/more/2.png">
            <figcaption>epoch 120</figcaption>
        </figure>
        <figure>
            <img src="media/more/4.png">
            <figcaption>epoch 200</figcaption>
        </figure>
        <figure>
            <img src="media/more/14.png">
            <figcaption>epoch 600</figcaption>
        </figure>
        <figure>
            <img src="media/more/24.png">
            <figcaption>epoch 1000</figcaption>
        </figure>
        <figure>
            <img src="media/more/74.png">
            <figcaption>epoch 3000</figcaption>
        </figure>
    </div>
    <h4>Bells & Whistles</h4>
    <p>In order to generate the background color, the formula for volume rendering needs to be modified. There are two mathematically equivalent solutions here. One is to directly use the probability that the light has not been terminated at the end as the weight of the background color, and the other is to sum up all other weights and use the remaining weight as the weight of the background color.</p>
    <p>To generate a depth map, we only need to normalize the distance from each sampling point to the camera and input it into the volume rendering function as a color. The reason is that if we choose the points with a termination probability higher than a certain threshold as the surface, it will obviously require quite detailed adjustments and take a lot of time. However, the Lego in this experiment does not have a transparent shell, so there is reason to believe that the non-termination probability will only change rapidly from 1 to 0 near the surface, so simply taking a certain average value is sufficient.</p>
    <p>Below are the videos generated after 1k and 3k epochs. If trained on Colab using a T4 GPU, the former takes a little over 10 minutes and the latter takes about 40 minutes.</p>
    <div class="gallery_2">
        <img src="media/lego_1000.gif">
        <img src="media/lego_dp_1000.gif">
        <img src="media/lego_3000.gif">
        <img src="media/lego_dp_3000.gif">
    </div>

</body>
</html>
